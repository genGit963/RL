{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f3bee3-4827-4cc0-b875-0222494af6c1",
   "metadata": {},
   "source": [
    "Tutorial: https://www.youtube.com/watch?v=q2ZOEFAaaI0&t=252s\n",
    "\n",
    "Taxiv2 based on Q_Learning that is based on Exploration and Exploitation Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58217a34-3e91-4060-951d-633fd7ef439f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3576ded9-a262-4631-a91d-f34011c342f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gymnasium as gym\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a58f1ae2-a1a4-4f0f-abe4-2e241a55d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37688c7d-3e92-4b45-bc7d-ad5c66608b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346, {'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8862c-3885-4f1f-9f30-0edc11a6f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " state:64 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:64 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:164 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:164 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:144 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:164 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:164 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:184 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:184 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:164 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:64 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:64 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:64 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:64 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:64 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:84 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)} \n",
      " state:84 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 1, 0], dtype=int8)} \n",
      " state:64 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 1, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:324 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:324 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:324 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:324 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:304 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:124 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:24 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 0, 1, 0, 0], dtype=int8)} \n",
      " state:4 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:4 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:104 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:44 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:144 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:224 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:304 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:404 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:404 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:404 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:304 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:304 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:304 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:204 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:324 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:224 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:144 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:244 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:244 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:384 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:484 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)} \n",
      " state:464 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:464 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:364 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 0, 0, 0], dtype=int8)} \n",
      " state:264 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:264 reward:-10 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 1, 1, 0, 0], dtype=int8)} \n",
      " state:284 reward:-1 done:False info:{'prob': 1.0, 'action_mask': array([1, 1, 0, 1, 0, 0], dtype=int8)} \n"
     ]
    }
   ],
   "source": [
    "# Testing environment \n",
    "episodes = 1\n",
    "for episode in range(1, episodes+1):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        print(\"\\r state:{} reward:{} done:{} info:{} \".format(state, reward, done, info))\n",
    "    print(\"Epi:{} score:{}\".format(episode, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688745c-8b02-40e4-8739-18786528e328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9fcb0fc-9289-4deb-9c0a-0d0f2be822e3",
   "metadata": {},
   "source": [
    "# Q-table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95871f9-d89f-4e5e-83c8-eff0774c925e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 500 Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# action_size and state_size\n",
    "action_size = env.action_space.n\n",
    "obs_size = env.observation_space.n\n",
    "print(action_size, obs_size, env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49323674-a21b-4be0-9fc4-6a854ca55150",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table = np.zeros((obs_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da417d-37cf-44fe-b82f-8adc7281e44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86b3236-3898-4452-8594-0327fd422657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb95042-d03c-49af-b98b-5399c7163240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de33e8e-aa09-48f6-8e6d-596f288cc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters \n",
    "total_episodes = 500000\n",
    "total_test_episodes = 100\n",
    "max_steps = 99\n",
    "\n",
    "learning_rate = 0.7\n",
    "gamma = 0.618\n",
    "\n",
    "#Exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.001\n",
    "decay_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4a2c02-d97b-4575-abba-802995c1bbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, {'prob': 1.0, 'action_mask': array([1, 0, 1, 0, 0, 0], dtype=int8)})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(state)\n",
    "state.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdb36830-37fa-4e6c-9e0d-5d87dfccf711",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "\n",
    "# after taking action, observe new_state and reward \n",
    "new_state, reward, done, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175cbf8b-a7d5-42e8-84ec-3efb5cbe91c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37b526-90c1-4b54-b5c5-20762752132f",
   "metadata": {},
   "source": [
    "# Exploration and Exploitation model and Updating Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498af266-8a7a-44f2-b053-5d7a098e17e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m new_state, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# update Q_value of Q_table : Bellman equation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m Q_table[state[\u001b[38;5;241m0\u001b[39m], action] \u001b[38;5;241m=\u001b[39m Q_table[state[\u001b[38;5;241m0\u001b[39m], action] \u001b[38;5;241m+\u001b[39m learning_rate\u001b[38;5;241m*\u001b[39m(reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q_table[new_state, :]) \u001b[38;5;241m-\u001b[39m Q_table[state[\u001b[38;5;241m0\u001b[39m],action])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# update current obs_state\u001b[39;00m\n\u001b[1;32m     26\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# learing model \n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    done = False\n",
    "    step = 0 \n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # print(\"----------top:{}---------\".format(step))\n",
    "        # action from current state\n",
    "        # first randomize \n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        if exp_exp_tradeoff > epsilon: # do exploitation ie taking biggest Q-value for this state\n",
    "            action = np.argmax(Q_table[state[0], :])\n",
    "        else: # do random choice : do exploration randomly the env\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # after taking action, observe new_state and reward \n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # update Q_value of Q_table : Bellman equation\n",
    "        Q_table[state[0], action] = Q_table[state[0], action] + learning_rate*(reward + gamma * np.max(Q_table[new_state, :]) - Q_table[state[0],action])\n",
    "        \n",
    "        # update current obs_state\n",
    "        state = new_state\n",
    "\n",
    "        if done == True: #breaking if taxi did\n",
    "            state = env.reset()\n",
    "            print(\"Episode:{}  Epsilon: {}, done:{} \\n\".format(episode, epsilon, done)) \n",
    "            break\n",
    "        \n",
    "        episode += 1\n",
    "\n",
    "        # reduce epsilon for less and less exploration and more exploitation \n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) *  np.exp(-decay_rate * episode)\n",
    "        \n",
    "        # print(\"----------end:{}---------\".format(step))\n",
    "\n",
    "print(\"Model Completed !! \\n Epsilon: {} \\n Q_Table:{}\".format(epsilon, Q_table))\n",
    "          \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49f5b40-3409-4e2a-bb50-75ee041d5f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4634113110390152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.4634113110390152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494cf9c6-d16e-4331-a4cd-0e2b6a9df03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 229  [ 0.   0.  -0.7  0.   0.   0. ] Sum: -0.7\n",
      "state: 341  [-0.7  0.   0.   0.   0.   0. ] Sum: -0.7\n",
      "\n",
      "\n",
      "--> The total trained Q_values are : 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trained Q_values count, q_count\n",
    "Q_count = 0\n",
    "for i in range(env.observation_space.n):\n",
    "    trainedRowSum = sum(Q_table[i,:])\n",
    "    if trainedRowSum != float(0):\n",
    "        print(\"state: {} \".format(i), Q_table[i,:], \"Sum: {}\".format(trainedRowSum))\n",
    "        Q_count += 1 \n",
    "        \n",
    "print(\"\\n\\n--> The total trained Q_values are : {}\\n\\n\".format(Q_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814de737-3a9f-4032-84f7-a4fa8ae9b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(obs_size):\n",
    "    print(\"state: {} \".format(i), Q_table[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2716b0-6559-4b35-aab8-b53e9f4a9e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a3307-a1ea-4206-8ac5-f241aff5c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d5516-1d19-4c7a-b6c1-e2be1c103907",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_table[state[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2ba18-20fa-444e-a08f-eb3f56bf610b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de61ec-ea99-43f7-8b76-95a388d1011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.argmax(Q_table[state[0],:])\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3836af3a-816a-4666-9048-3e699d593370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing with eplore and expoit \n",
    "\n",
    "rewards = []\n",
    "for episode in range(int(0.1*total_test_episodes)):\n",
    "    \n",
    "    done = False \n",
    "    step = 0\n",
    "    score = 0\n",
    "    print(\"-------------------------------------------------------\\n\", \"Episode: {}\".format(episode), \"\\n\")\n",
    "    for step in range(int(0.2*max_steps)):\n",
    "        env.render()\n",
    "        print(state)\n",
    "        state_i=state[0]\n",
    "        action = np.argmax(Q_table[state_i,:]) # action using Q_table as pie-policy/ state-action policy\n",
    "        \n",
    "        new_state, reward, done , truncated, info = env.step(action)\n",
    "        score += reward # total_rewards\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            rewards.append(score)\n",
    "            print(\"Score: {}\".format(score))\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "print(\"Score over time: {}\".format((sum(rewards)/total_test_episodes)))\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10a9af-c0c9-454b-b4ff-3b56874700fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f584d3-dfa0-4a6c-85bf-e0fe34a84be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
