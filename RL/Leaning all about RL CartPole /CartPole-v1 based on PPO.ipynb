{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b904a3",
   "metadata": {},
   "source": [
    "## 1. Installing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0be16a-1d40-4b8c-babc-c19f9f49ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'stable-baselines3[extra]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2941bf8a-dd4c-46ef-9b71-2bd03b63ddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.5\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c87ea8c-85fa-4755-b7eb-ca72dc3ef4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd13134b-5741-4318-8a28-ee697c4b2fbf",
   "metadata": {},
   "source": [
    "## 2. Load Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb4d9ff-6e0a-459d-9b07-1d38d07bfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da352e3-9795-45a4-90b9-022f40461086",
   "metadata": {},
   "source": [
    "Test out the environment and actually loading the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38874575-8731-4ab2-9d3b-df862d22bcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 1  Score: 51.0\n",
      "Episodes: 2  Score: 14.0\n",
      "Episodes: 3  Score: 10.0\n",
      "Episodes: 4  Score: 14.0\n",
      "Episodes: 5  Score: 42.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "        # print('Episode:{}, Score :{} \\n ---> n_state:{} \\n ---> done:{}, truncated:{}, info:{}'.format(episode, score, n_state, done, truncated, info))\n",
    "    print(\"Episodes:\", episode, \" Score:\", score)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054decd-6e00-4503-b09d-b2f6432dfb5b",
   "metadata": {},
   "source": [
    "2.1 Understanding the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c78d7c9-2fb0-4731-8c69-cd84ebc253f7",
   "metadata": {},
   "source": [
    "ACTION_SPACE: of pole\n",
    "0: Push cart to the left\n",
    "1: Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d7a77e-de03-45ae-832a-66f0026c3ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space:  Discrete(2)\n",
      "action_space.sample():  0\n"
     ]
    }
   ],
   "source": [
    "print( \"action_space: \", env.action_space)\n",
    "print( \"action_space.sample(): \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7cedd-ccf9-4697-8180-04a3eb9be4dc",
   "metadata": {},
   "source": [
    "OBSERVATION_SPACE : CartPole \n",
    "Box(4,)\n",
    "0: Cart_position\n",
    "1: Cart_Velocity\n",
    "2: Pole_angle\n",
    "3: Pole_Angular_Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb772826-aa53-4ba8-915a-721c2cc793de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs_space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "obs_space.sample(): [ 4.4520354e+00  2.0612396e+37 -2.4021810e-01 -2.4867722e+37]\n"
     ]
    }
   ],
   "source": [
    "print( \"obs_space:\", env.observation_space)\n",
    "print( \"obs_space.sample():\", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34717aff-1be4-4ccf-8141-158502e2d03d",
   "metadata": {},
   "source": [
    "## 3. Train the RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c762e0ef-5089-4708-b088-6f7f275c1db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directories first training/logs\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7c80fde-c12c-4cfd-95f0-6d6d194509d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efea53d-8368-465a-906d-8dd17e6aa484",
   "metadata": {},
   "source": [
    "Setting up the RL_Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e431fe4a-fd01-469b-a76a-71d8a7de078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda:env])\n",
    "\n",
    "#choosed apprioprate model \n",
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482ae2e-0d0e-4635-9f36-ba0924967709",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6179234-3d80-479d-8afa-a36c7bab5bee",
   "metadata": {},
   "source": [
    "Executing the Agent Learning, Model Training or  Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d2b557d-7cfa-482c-a402-909d7556ead2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_12\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 4912 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 3344       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00998391 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.685     |\n",
      "|    explained_variance   | -0.00613   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.81       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    value_loss           | 42.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3082        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063039 |\n",
      "|    clip_fraction        | 0.0514      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 37.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2968        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008171255 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 50.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2897         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071176337 |\n",
      "|    clip_fraction        | 0.0716       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.61        |\n",
      "|    explained_variance   | 0.299        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24           |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0176      |\n",
      "|    value_loss           | 62.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2849        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009476215 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    value_loss           | 58.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2816         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068859807 |\n",
      "|    clip_fraction        | 0.0498       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.575       |\n",
      "|    explained_variance   | 0.463        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 39.7         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00974     |\n",
      "|    value_loss           | 67.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2782         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071508912 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.45         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00585     |\n",
      "|    value_loss           | 34.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2766        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005925992 |\n",
      "|    clip_fraction        | 0.047       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00785    |\n",
      "|    value_loss           | 52.1        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2753          |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071877765 |\n",
      "|    clip_fraction        | 0.00356       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.565        |\n",
      "|    explained_variance   | 0.751         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.88          |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00218      |\n",
      "|    value_loss           | 26.4          |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x17a0a95d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## executing the learning or model training  or training the agent\n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6364863-54f8-4279-b65c-eda4ed476bc0",
   "metadata": {},
   "source": [
    "## 4. Save and Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70487d92-d081-42b0-a1d0-fc48d1463433",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the model \n",
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO-CartPole-model-v0')\n",
    "model.save(PPO_Path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "275a6bc3-f970-4e6e-8b05-1086500f27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying delete then reloading the saved model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "218de426-20e4-4fc9-a097-4dffc2a6d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloading\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d67ed-4c50-4ca4-841c-8b19a1bc29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if loaded or not\n",
    "# model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "860dc23a-0478-4510-a561-d9aa6f9b5aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training/Logs/PPO_15\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 45   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 45   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038913414 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.561       |\n",
      "|    explained_variance   | 0.729        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.503        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    value_loss           | 26           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008450247 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.26        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007208842 |\n",
      "|    clip_fraction        | 0.0814      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.147       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 218          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021981588 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.537       |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.2         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    value_loss           | 48.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008027541 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | -0.226      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0904      |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00268    |\n",
      "|    value_loss           | 1.31        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 305         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006886912 |\n",
      "|    clip_fraction        | 0.0485      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.0364      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0341      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 0.844       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 46         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 348        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00754686 |\n",
      "|    clip_fraction        | 0.0425     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.505     |\n",
      "|    explained_variance   | -0.00873   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.177      |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.00426   |\n",
      "|    value_loss           | 0.777      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 392         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009039966 |\n",
      "|    clip_fraction        | 0.0756      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | -0.158      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0271      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00412    |\n",
      "|    value_loss           | 0.345       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 437          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053984304 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.496       |\n",
      "|    explained_variance   | 0.314        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 0.211        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x292923b90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reusing the trained agent or model \n",
    "model.learn(total_timesteps= 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa749958-3ce7-4ecb-b7d2-077246d6505c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e962b57-1996-4ec4-a717-75e8454a0ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fc2709c-60be-4206-95f2-ecec294514d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50de01d-8ce6-40cf-a8a5-e0ddf234ba54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c215d195-6a3c-45ee-95dd-1eb42cdf8284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Episode:1 =>  [0] [[-0.02599332 -0.18074583 -0.02502888  0.32777467]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[-0.02960824 -0.37550268 -0.01847339  0.61246073]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.03711829 -0.18012749 -0.00622417  0.3140171 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-4.0720843e-02  1.5082573e-02  5.6170313e-05  1.9377774e-02]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[-0.04041919 -0.18004018  0.00044373  0.31207842]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[-0.04402    -0.37516844  0.00668529  0.60490125]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.05152337 -0.18014063  0.01878332  0.3143315 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.05512618  0.01470879  0.02506995  0.02763092]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.054832    0.20946242  0.02562257 -0.25703794]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.05064275  0.40420935  0.02048181 -0.54153025]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[-0.04255857  0.2088056   0.0096512  -0.24246487]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.03838245  0.4037884   0.00480191 -0.532088  ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.03030669  0.59884244 -0.00583985 -0.823254  ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.01832984  0.79404384 -0.02230493 -1.1177679 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[-0.00244896  0.9894512  -0.04466029 -1.4173633 ]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[ 0.01734006  0.79490983 -0.07300755 -1.1389679 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[ 0.03323826  0.9909065  -0.09578691 -1.4536248 ]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[ 0.05305639  0.7970824  -0.12485941 -1.1923398 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[ 0.06899804  0.99358076 -0.14870621 -1.5214062 ]] [1.] [False]\n",
      "--> Episode:1 =>  [1] [[ 0.08886965  1.1901537  -0.17913432 -1.8565701 ]] [1.] [False]\n",
      "--> Episode:1 =>  [0] [[ 0.00815312 -0.02491952  0.0172581   0.01964027]] [1.] [ True]\n",
      "Episdoe:1, Score:[21.]\n",
      "--> Episode:2 =>  [1] [[ 0.01345677  0.24008623  0.04367824 -0.30068633]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[0.01825849 0.04436981 0.03766451 0.00544561]] [1.] [False]\n",
      "--> Episode:2 =>  [1] [[ 0.01914589  0.23893192  0.03777343 -0.27511984]] [1.] [False]\n",
      "--> Episode:2 =>  [1] [[ 0.02392452  0.43349513  0.03227103 -0.55565375]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 0.03259443  0.23793532  0.02115796 -0.25298068]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[0.03735314 0.04251774 0.01609834 0.04630004]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 0.03820349 -0.1528313   0.01702434  0.34401834]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 0.03514686 -0.34819126  0.02390471  0.6420207 ]] [1.] [False]\n",
      "--> Episode:2 =>  [1] [[ 0.02818304 -0.15341052  0.03674512  0.35696042]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 0.02511483 -0.3490351   0.04388433  0.66099983]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 0.01813412 -0.54473937  0.05710433  0.96717143]] [1.] [False]\n",
      "--> Episode:2 =>  [1] [[ 0.00723934 -0.35042882  0.07644776  0.69296026]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[ 2.3076210e-04 -5.4652339e-01  9.0306967e-02  1.0086968e+00]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[-0.01069971 -0.74272716  0.1104809   1.3283174 ]] [1.] [False]\n",
      "--> Episode:2 =>  [1] [[-0.02555425 -0.54915917  0.13704725  1.072151  ]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[-0.03653743 -0.7458003   0.15849027  1.4045129 ]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[-0.05145344 -0.9424951   0.18658052  1.7422585 ]] [1.] [False]\n",
      "--> Episode:2 =>  [0] [[-0.04304864  0.00842629  0.01793348  0.03618141]] [1.] [ True]\n",
      "Episdoe:2, Score:[18.]\n",
      "--> Episode:3 =>  [1] [[-0.0063819   0.19988608 -0.03437487 -0.33433756]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[-0.00238418  0.39547998 -0.04106162 -0.6376593 ]] [1.] [False]\n",
      "--> Episode:3 =>  [0] [[ 0.00552542  0.20095395 -0.05381481 -0.35818493]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[ 0.0095445   0.396798   -0.06097851 -0.6673397 ]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[ 0.01748046  0.5927126  -0.0743253  -0.97858196]] [1.] [False]\n",
      "--> Episode:3 =>  [0] [[ 0.02933471  0.39866158 -0.09389694 -0.7101405 ]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[ 0.03730794  0.59494984 -0.10809975 -1.0308411 ]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[ 0.04920694  0.79133093 -0.12871657 -1.3554128 ]] [1.] [False]\n",
      "--> Episode:3 =>  [0] [[ 0.06503356  0.59803766 -0.15582483 -1.1056097 ]] [1.] [False]\n",
      "--> Episode:3 =>  [0] [[ 0.07699431  0.4052691  -0.17793703 -0.86558753]] [1.] [False]\n",
      "--> Episode:3 =>  [1] [[ 0.08509969  0.60230833 -0.19524878 -1.2085177 ]] [1.] [False]\n",
      "--> Episode:3 =>  [0] [[-0.03915609  0.00039507  0.01679894  0.01733187]] [1.] [ True]\n",
      "Episdoe:3, Score:[12.]\n",
      "--> Episode:4 =>  [0] [[-0.01693984 -0.16109684  0.01382183  0.31735176]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.02016177  0.03382555  0.02016887  0.0290596 ]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.01948526 -0.16157974  0.02075006  0.32803717]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.02271686  0.03324075  0.0273108   0.04196942]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.02205204 -0.16226198  0.02815019  0.3431425 ]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.02529728 -0.35777286  0.03501304  0.6445677 ]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.03245274 -0.1631559   0.04790439  0.36311293]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.03571586  0.03125362  0.05516665  0.08591168]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.03509078  0.22554319  0.05688488 -0.18886854]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.03057992  0.02965549  0.05310751  0.12120288]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.02998681  0.22397792  0.05553157 -0.15426356]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.02550725  0.02810665  0.0524463   0.15540835]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.02494512  0.22243997  0.05555447 -0.12027869]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.02049632  0.4167238   0.0531489  -0.39493018]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.01216184  0.22088957  0.04525029 -0.08597523]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.00774405  0.41533467  0.04353078 -0.36404526]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[ 0.00056264  0.21962196  0.03624988 -0.05796029]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[0.00495508 0.0239995  0.03509067 0.24593574]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[ 0.00543507 -0.1716056   0.04000939  0.54947734]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[ 0.00200296 -0.36726606  0.05099894  0.8544927 ]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.00534236 -0.5630446   0.06808879  1.1627659 ]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.01660325 -0.7589839   0.09134411  1.4759965 ]] [1.] [False]\n",
      "--> Episode:4 =>  [0] [[-0.03178293 -0.95509535  0.12086404  1.7957551 ]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.05088484 -0.7615165   0.15677914  1.5429527 ]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.06611517 -0.56858784  0.1876382   1.3030134 ]] [1.] [False]\n",
      "--> Episode:4 =>  [1] [[-0.04892916 -0.03469948 -0.02442988  0.00716996]] [1.] [ True]\n",
      "Episdoe:4, Score:[26.]\n",
      "--> Episode:5 =>  [1] [[ 0.01222329  0.2440237   0.02558354 -0.30827388]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.01710376  0.43877196  0.01941807 -0.59277993]] [1.] [False]\n",
      "--> Episode:5 =>  [0] [[ 0.0258792   0.24338363  0.00756247 -0.29404414]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.03074687  0.43839696  0.00168159 -0.5843324 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.03951481  0.63349533 -0.01000506 -0.8764851 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.05218472  0.8287518  -0.02753476 -1.1722966 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.06875975  1.0242207  -0.0509807  -1.4734831 ]] [1.] [False]\n",
      "--> Episode:5 =>  [0] [[ 0.08924416  0.82975763 -0.08045036 -1.1971493 ]] [1.] [False]\n",
      "--> Episode:5 =>  [0] [[ 0.10583932  0.63576376 -0.10439334 -0.9307266 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.1185546   0.8321279  -0.12300788 -1.2543052 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.13519715  1.0285915  -0.14809398 -1.5828471 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[ 0.15576899  1.2251327  -0.17975092 -1.9178125 ]] [1.] [False]\n",
      "--> Episode:5 =>  [1] [[-0.000147   -0.02808066 -0.03436203  0.00772435]] [1.] [ True]\n",
      "Episdoe:5, Score:[13.]\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _state = model.predict(obs) # using saved model \n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        print( \"--> Episode:{} => \".format(episode), action, obs, reward, done )\n",
    "    print(\"Episdoe:{}, Score:{}\".format(episode, score))\n",
    "\n",
    "env.close()   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462ddd3-f6fd-48d3-9d62-b40e573001ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Viewing the logs in Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4d6141-6204-4330-85de-617e04eb1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f2d069-a243-4e1e-9185-3fb3d20bdb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Logs/PPO_14'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0d0baf-82f7-472e-bb4c-7fc8efe47a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'Training',\n",
       " 'CartPole-v1 based on PPO.ipynb',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011c0c3-f4df-4016-9ee3-aa7df4f309b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.14.1 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "#viewing in the tensor_board \n",
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f05cb1-13c4-4f9f-9442-61f3c7f89a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de17f8b7-5070-41fa-a3ea-990f4928aedc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Adding to callback to the training stages : BenchMarking wrt Reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ecd449-acbc-441a-ac9a-01f069992f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training/Saved Models'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "493d0f38-936a-4280-90e9-ac2ecca497a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stop_callback \u001b[38;5;241m=\u001b[39m StopTrainingOnRewardThreshold(reward_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\u001b[43menv\u001b[49m,\n\u001b[1;32m      3\u001b[0m                              callback_on_new_best \u001b[38;5;241m=\u001b[39m stop_callback,\n\u001b[1;32m      4\u001b[0m                              eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      5\u001b[0m                              best_model_save_path\u001b[38;5;241m=\u001b[39msave_path,\n\u001b[1;32m      6\u001b[0m                              verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=250, verbose=1) # setting stop benchmark with reward:250\n",
    "eval_callback = EvalCallback(env,\n",
    "                             callback_on_new_best = stop_callback,\n",
    "                             eval_freq=10000,\n",
    "                             best_model_save_path=save_path,\n",
    "                             verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bd77a-699c-4f9a-b037-1b059cdce809",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be19ec86-24ba-4de0-8cfc-21b6c242ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing the agent training from the previous state: eval_callback\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9ef80-0f6b-4ec9-9a3c-2e9c0c414b70",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. Changing the Policies\n",
    "it means changing the neural network for PPO model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785cda57-aa41-446c-878b-c96e0e9e6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = [dict(pi=[128, 128, 128, 128], vf=[128, 128, 128, 128])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93d6e3b-34c5-4134-822a-b6cad0738bd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43menv\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_path, policy_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnet_arch\u001b[39m\u001b[38;5;124m'\u001b[39m:net_arch})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path, policy_kwargs={'net_arch':net_arch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd88ee5-446a-4dcb-9717-b86e86b663a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model` not found.\n"
     ]
    }
   ],
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef747a-660d-461e-ac4b-b2f3ba8554f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retraining againg with changed policy\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f91382-db40-4004-b88a-5ca680a3349e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. Using Alternative Algorithm: for this DQN from PPO\n",
    " if used model doesn't provides optimal metrics and performance evaluation then change Algorthim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c78eb41-20f3-49ee-9ad6-c01225e65447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import alternative model \n",
    "from stable_baselines3 import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f059f12-5cb4-4010-8ae2-a7f883c66b18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m DQN(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43menv\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_path) \u001b[38;5;66;03m# intansitating the alternative model \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "3model = DQN('MlpPolicy', env, verbose=1, tensorboard_log=log_path) # intansitating the alternative model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b98544a-eb9f-43dd-8c30-ada4c3ab9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training new model \n",
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bcc0b-b61a-4de1-8df8-6c5e2396261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the alternative model \n",
    "model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
